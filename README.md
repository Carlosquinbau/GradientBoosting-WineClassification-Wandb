# GradientBoosting-WineClassification-Wandb


Este repositorio contiene un proyecto de experimentaci칩n de ciencia de datos en Upgrade Hub. Utilizamos pesos y sesgos para ajustar y evaluar sistem치ticamente los hiperpar치metros de un clasificador de aumento de gradiente. El DataSet con el que estamos trabajando es el de Wine.

## Sobre Upgrade Hub

[Upgrade Hub](https://www.upgrade-hub.com/) Es una instituci칩n educativa dedicada a ofrecer programas de capacitaci칩n de alta calidad en tecnolog칤a y ciencia de datos.

## Resultados

Los resultados de la experimentaci칩n se pueden ver en detalle en siguiente enlace de WanDB. Esto incluye varias m칠tricas de rendimiento, como la precisi칩n, junto con los hiperpar치metros utilizados para cada experimento.

游댕 [View Experimentation Results](https://api.wandb.ai/links/carlos-quintero-bautista/x31ys56l)

## Descripci칩n del Problema

La tarea que nos ocupa es un ejemplo cl치sico de un problema de clasificaci칩n multiclase. Nuestro objetivo es predecir la categor칤a de vino en funci칩n de varios atributos fisicoqu칤micos. El conjunto de datos de Wine es un conjunto de datos de referencia com칰n en la comunidad de aprendizaje autom치tico.

## Dataset

El conjunto de datos Wine es un conjunto de datos disponible p칰blicamente que contiene 178 muestras de vinos con 13 atributos diferentes, como contenido de alcohol, 치cido m치lico, cenizas, etc. Hay tres clases que representan tres tipos diferentes de vinos. El conjunto de datos es muy adecuado para experimentos de clasificaci칩n.

## Experimentation

For this project, we utilize the Gradient Boosting Classifier, a powerful ensemble machine learning algorithm that builds on decision trees. It is particularly known for its effectiveness in classification problems.

To find the best model, we explore various combinations of hyperparameters such as learning rate, maximum depth of the trees, the number of estimators, etc. Through systematic experimentation, we aim to understand the effect of these hyperparameters on the model's performance and find the combination that yields the best results.

We integrate Weights & Biases into our experimentation pipeline, which allows us to log the hyperparameters and the performance metrics for each experiment. Weights & Biases provides us with an interactive dashboard where we can visualize and analyze the results.

## Hyperparameter Tuning and Best Model

During the experimentation process, we performed an extensive search over the hyperparameter space. A total of **384 different combinations** of hyperparameters were tested to find the model that yields the best performance. The hyperparameters that we tuned include:

- Learning rate
- Maximum depth of the trees
- Number of estimators
- Loss function
- Subsample fraction
- Minimum number of samples required to split an internal node
- Minimum number of samples required to be at a leaf node

This extensive search allowed us to explore a wide range of models and identify the combination of hyperparameters that optimizes the performance for this specific dataset.

The model with the best score achieved an accuracy of **0.9815**. This high level of accuracy indicates that the model is highly effective in classifying the wine samples correctly. The hyperparameters of the best model are as follows:

- Learning rate: 0.1
- Loss function: deviance
- Max depth: 3
- min_samples_leaf: 2
- min_samples_split: 2
- n_estimators: 50
- subsample: 1

This combination of hyperparameters allowed the Gradient Boosting Classifier to capture the underlying patterns in the data efficiently and make highly accurate predictions.

## Running the Code

To run the code, first, ensure you have all the dependencies installed:


## Contributions

Contributions to this repository are welcome. Please, ensure that the code follows best practices and is well-documented.

## License

This project is licensed under the MIT License.
